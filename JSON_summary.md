Step 1 â€” Setup & Verify Environment + Dataset: Complete âœ…

```json
{
  "status": "ok",
  "stdout": "ðŸ“‚ Loading data from data\\raw\\Telco-Customer-Churn.csv\n\nðŸ“Š Dataset Information:\n   Shape: (7043, 21) (rows: 7043, columns: 21)\n\nðŸ“‹ Columns (21):\n    1. customerID\n    2. gender\n    3. SeniorCitizen\n    4. Partner\n    5. Dependents\n    6. tenure\n    7. PhoneService\n    8. MultipleLines\n    9. InternetService\n   10. OnlineSecurity\n   11. OnlineBackup\n   12. DeviceProtection\n   13. TechSupport\n   14. StreamingTV\n   15. StreamingMovies\n   16. Contract\n   17. PaperlessBilling\n   18. PaymentMethod\n   19. MonthlyCharges\n   20. TotalCharges\n   21. Churn\n\nðŸ” Missing Values Count:\n   No missing values found! âœ…\n\nðŸ’¾ Saved first 100 rows to data\\processed\\sample.csv\n   Sample shape: (100, 21)",
  "errors": "",
  "fixes_made": [
    ".venv -> Recreated virtual environment due to corrupted path reference"
  ],
  "next_step": "Step 2 - EDA & columns.json"
}
```

---

## Step 2 â€” Run EDA and Produce columns.json: Complete âœ…

```json
{
  "status": "ok",
  "stdout": "ðŸ“‚ Loading data from data\\raw\\Telco-Customer-Churn.csv\n   Dataset shape: (7043, 21)\n\nðŸ”§ Data Cleaning:\n   TotalCharges original type: object\n   âš ï¸  Found 11 non-numeric values in TotalCharges (converted to NaN)\n   TotalCharges new type: float64\n\nðŸ“Š Churn Distribution:\n   No: 5,174 (73.46%)\n   Yes: 1,869 (26.54%)\n   Churn Rate: 26.54%\n\nðŸ” Column Type Detection:\n   Numeric columns (4):\n      â€¢ SeniorCitizen (int64)\n      â€¢ tenure (int64)\n      â€¢ MonthlyCharges (float64)\n      â€¢ TotalCharges (float64)\n   Categorical columns (15):\n      â€¢ gender (2 unique values)\n      â€¢ Partner (2 unique values)\n      â€¢ Dependents (2 unique values)\n      â€¢ PhoneService (2 unique values)\n      â€¢ MultipleLines (3 unique values)\n      â€¢ InternetService (3 unique values)\n      â€¢ OnlineSecurity (3 unique values)\n      â€¢ OnlineBackup (3 unique values)\n      â€¢ DeviceProtection (3 unique values)\n      â€¢ TechSupport (3 unique values)\n      â€¢ StreamingTV (3 unique values)\n      â€¢ StreamingMovies (3 unique values)\n      â€¢ Contract (3 unique values)\n      â€¢ PaperlessBilling (2 unique values)\n      â€¢ PaymentMethod (4 unique values)\n\nðŸ’¾ Saved column metadata to data\\processed\\columns.json\n   Summary: 4 numeric, 15 categorical columns",
  "columns_json_valid": true,
  "numeric_cols_count": 4,
  "categorical_cols_count": 15,
  "fixes_made": [
    "src/data/eda.py -> Updated to output numeric_cols and categorical_cols as top-level keys in JSON"
  ],
  "next_step": "Step 3 - Preprocessor build"
}
```

### Summary of Step 2:

âœ… **EDA Script Execution**: Successfully ran `src/data/eda.py`
âœ… **TotalCharges Conversion**: Converted from object to float64, found 11 non-numeric values (converted to NaN)
âœ… **Churn Distribution**: 
  - No: 5,174 (73.46%)
  - Yes: 1,869 (26.54%)
  - Churn Rate: 26.54%
âœ… **Column Detection**:
  - Numeric columns (4): SeniorCitizen, tenure, MonthlyCharges, TotalCharges
  - Categorical columns (15): gender, Partner, Dependents, PhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling, PaymentMethod
âœ… **columns.json**: Created and validated with required keys `numeric_cols` and `categorical_cols`

---

## Step 3  Build and Save scikit-learn Preprocessor: Complete 

```json
{
  \"status\": \"ok\",
  \"transform_shape\": \"(5, 45)\",
  \"errors\": \"\",
  \"fixes_made\": [
    \"src/data/preprocess.py -> Updated to handle both 'numeric_cols'/'categorical_cols' and nested 'columns' JSON formats\"
  ],
  \"next_step\": \"Step 4 - Train sklearn model\"
}
```

### Summary of Step 3

 **Preprocessor Script**: Found and updated src/data/preprocess.py to handle correct JSON format
 **Preprocessor Build**:
  - Successfully fitted preprocessor on 7,043 samples
  - Input features: 19 (4 numeric + 15 categorical)
  - Output features: 45 (after one-hot encoding)
  - Saved to: artifacts/models/preprocessor.joblib
 **Pipeline Components**:
  - Numeric Pipeline: SimpleImputer(strategy='median')  StandardScaler()
  - Categorical Pipeline: SimpleImputer(strategy='constant', fill_value='missing')  OneHotEncoder(handle_unknown='ignore', sparse_output=False)
 **Transform Validation**: Successfully tested transform on first 5 rows, output shape: (5, 45)
 **Feature Names**: Saved to artifacts/models/feature_names.json (45 total features)

---
## Step 4  Train scikit-learn Model and Validate Output Files: Complete 

```json
{
  \"status\": \"ok\",
  \"metrics\": {
    \"accuracy\": 0.8006,
    \"roc_auc\": 0.8466
  },
  \"model_file\": \"artifacts/models/sklearn_pipeline.joblib\",
  \"fixes_made\": [
    \"src/models/train.py -> Updated to handle both 'numeric_cols'/'categorical_cols' and nested 'columns' JSON formats\"
  ],
  \"next_step\": \"Step 5 - Inference API & tests\"
}
```

### Summary of Step 4

 **Training Script**: Found and updated src/models/train.py to handle correct JSON format
 **Data Preparation**:
  - Loaded 7,043 samples (5,634 train / 1,409 test)
  - Stratified split maintaining 26.54% churn rate
  - Converted TotalCharges to numeric (11 values coerced to NaN)
 **Model Training**:
  - Algorithm: GradientBoostingClassifier
  - Parameters: n_estimators=100, learning_rate=0.05, max_depth=3, subsample=0.8
  - Input features: 19 (4 numeric + 15 categorical)
  - Transformed features: 45 (after preprocessing)
 **Performance Metrics**:
  - Training Accuracy: 0.8158 (81.58%)
  - Training ROC AUC: 0.8669 (86.69%)
  - **Test Accuracy: 0.8006 (80.06%)**
  - **Test ROC AUC: 0.8466 (84.66%)**
 **Artifacts Created**:
  - Model: artifacts/models/sklearn_pipeline.joblib
  - Metrics: artifacts/metrics/sklearn_metrics.json
 **Model Quality**: ROC AUC of 0.8466 indicates strong predictive performance (well above 0.6 threshold)

---
