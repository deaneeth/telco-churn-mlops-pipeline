{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18162886",
   "metadata": {},
   "source": [
    "# Telco Customer Churn - Feature Engineering\n",
    "\n",
    "## Purpose and Objectives\n",
    "\n",
    "This notebook focuses on feature engineering and preprocessing for the Telco Customer Churn prediction model. Based on insights from our exploratory data analysis, we will:\n",
    "\n",
    "### Why Feature Engineering is Critical for Churn Prediction:\n",
    "\n",
    "**ðŸŽ¯ Data Quality Issues**: Our EDA revealed that `TotalCharges` contains string values that need proper conversion to numeric format for machine learning algorithms.\n",
    "\n",
    "**ðŸ”„ Mixed Data Types**: The dataset contains both numerical features (tenure, charges) and categorical features (contract type, services) that require different preprocessing approaches.\n",
    "\n",
    "### Key Steps in This Notebook:\n",
    "\n",
    "1. **Data Loading & Cleaning**: Fix data quality issues identified in EDA\n",
    "2. **Feature Type Separation**: Distinguish numerical vs categorical features\n",
    "3. **Preprocessing Pipeline Development**: Create robust sklearn pipelines\n",
    "4. **Data Transformation**: Apply preprocessing to training data\n",
    "5. **Artifact Saving**: Store configurations for production use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2b4fb",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ed9e22fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Sklearn preprocessing and model selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4fae0ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure display and plotting\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de15af",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3cfc7c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully - Shape: (7043, 21)\n",
      "Target variable: Churn\n"
     ]
    }
   ],
   "source": [
    "# Load the raw dataset\n",
    "data_path = Path('../data/raw/Telco-Customer-Churn.csv')\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset loaded successfully - Shape: {df.shape}\")\n",
    "print(f\"Target variable: Churn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1e2dfcaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7043, 21)\n",
      "Target distribution: {'No': 5174, 'Yes': 1869}\n"
     ]
    }
   ],
   "source": [
    "# Basic dataset information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution: {df['Churn'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b21d46f",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Based on our EDA findings, we need to address several data quality issues before feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "338e5efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalCharges original type: object\n",
      "Found 11 customers with empty TotalCharges\n"
     ]
    }
   ],
   "source": [
    "# Convert TotalCharges to numeric (identify empty string values)\n",
    "print(f\"TotalCharges original type: {df['TotalCharges'].dtype}\")\n",
    "empty_totalcharges = df[df['TotalCharges'] == ' ']\n",
    "print(f\"Found {len(empty_totalcharges)} customers with empty TotalCharges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "316772dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TotalCharges converted to: float64\n",
      "Missing values created: 11\n"
     ]
    }
   ],
   "source": [
    "# Convert TotalCharges to numeric (empty strings become NaN)\n",
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "print(f\"TotalCharges converted to: {df['TotalCharges'].dtype}\")\n",
    "print(f\"Missing values created: {df['TotalCharges'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "978fbacb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values found:\n",
      "                    Column  Missing_Count  Missing_Percentage\n",
      "TotalCharges  TotalCharges             11            0.156183\n"
     ]
    }
   ],
   "source": [
    "# Analyze missing values\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing_Count': df.isnull().sum(),\n",
    "    'Missing_Percentage': (df.isnull().sum() / len(df)) * 100\n",
    "})\n",
    "\n",
    "missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "if len(missing_summary) > 0:\n",
    "    print(\"Missing values found:\")\n",
    "    print(missing_summary)\n",
    "else:\n",
    "    print(\"No missing values found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f4f4301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: 21\n",
      "Final shape after removing customerID: (7043, 20)\n"
     ]
    }
   ],
   "source": [
    "# Remove identifier columns\n",
    "print(f\"Original columns: {df.shape[1]}\")\n",
    "df_clean = df.drop(columns=['customerID'])\n",
    "print(f\"Final shape after removing customerID: {df_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865e06d",
   "metadata": {},
   "source": [
    "## Feature Type Identification\n",
    "\n",
    "We need to separate features into numerical and categorical types for appropriate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a0f50943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features (4): ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "Categorical features (15): ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "Total features: 19\n"
     ]
    }
   ],
   "source": [
    "# Identify numerical and categorical columns\n",
    "# Get all columns except target\n",
    "all_features = [col for col in df_clean.columns if col != 'Churn']\n",
    "\n",
    "# Automatically detect numerical columns\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if 'Churn' in numerical_cols:\n",
    "    numerical_cols.remove('Churn')\n",
    "\n",
    "# Automatically detect categorical columns  \n",
    "categorical_cols = df_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'Churn' in categorical_cols:\n",
    "    categorical_cols.remove('Churn')\n",
    "\n",
    "print(f\"Numerical features ({len(numerical_cols)}): {numerical_cols}\")\n",
    "print(f\"Categorical features ({len(categorical_cols)}): {categorical_cols}\")\n",
    "print(f\"Total features: {len(numerical_cols) + len(categorical_cols)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2334133f",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline Development\n",
    "\n",
    "We'll create sklearn pipelines to handle numerical and categorical features differently, ensuring robust preprocessing for both training and production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9de79cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical pipeline created\n"
     ]
    }
   ],
   "source": [
    "# Create numerical preprocessing pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "print(\"Numerical pipeline created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fd675307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical pipeline created\n"
     ]
    }
   ],
   "source": [
    "# Create categorical preprocessing pipeline  \n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "    ('encoder', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "print(\"Categorical pipeline created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c6fb854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined preprocessor created: 4 numerical + 15 categorical features\n"
     ]
    }
   ],
   "source": [
    "# Combine pipelines with ColumnTransformer\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('numerical', numerical_pipeline, numerical_cols),\n",
    "    ('categorical', categorical_pipeline, categorical_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "print(f\"Combined preprocessor created: {len(numerical_cols)} numerical + {len(categorical_cols)} categorical features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7db32a",
   "metadata": {},
   "source": [
    "## Save Column Configuration\n",
    "\n",
    "We'll save the feature column lists to a JSON file for reuse in training and production scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0aa5f374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column configuration prepared: 4 numerical + 15 categorical features\n"
     ]
    }
   ],
   "source": [
    "# Create processed data directory and column configuration\n",
    "processed_dir = Path('../data/processed')\n",
    "processed_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Prepare column configuration for saving\n",
    "column_config = {\n",
    "    'columns': {\n",
    "        'numerical': numerical_cols,\n",
    "        'categorical': categorical_cols,\n",
    "        'all_features': numerical_cols + categorical_cols,\n",
    "        'target': 'Churn'\n",
    "    },\n",
    "    'metadata': {\n",
    "        'total_features': len(numerical_cols) + len(categorical_cols),\n",
    "        'numerical_count': len(numerical_cols),\n",
    "        'categorical_count': len(categorical_cols),\n",
    "        'dataset_shape': df_clean.shape,\n",
    "        'preprocessing_steps': {\n",
    "            'numerical': ['median_imputation', 'standard_scaling'],\n",
    "            'categorical': ['constant_imputation', 'one_hot_encoding']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Column configuration prepared: {len(numerical_cols)} numerical + {len(categorical_cols)} categorical features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "662b16a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column configuration saved to: ..\\data\\processed\\columns.json\n"
     ]
    }
   ],
   "source": [
    "# Save column configuration to JSON file\n",
    "columns_file = processed_dir / 'columns.json'\n",
    "with open(columns_file, 'w') as f:\n",
    "    json.dump(column_config, f, indent=2)\n",
    "\n",
    "print(f\"Column configuration saved to: {columns_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cee907c",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "\n",
    "We'll create stratified train-test splits to ensure balanced representation of churn classes in both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1a55bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (7043, 19)\n",
      "Target distribution: {'No': 5174, 'Yes': 1869}\n"
     ]
    }
   ],
   "source": [
    "# Prepare features and target\n",
    "all_feature_cols = numerical_cols + categorical_cols\n",
    "X = df_clean[all_feature_cols]\n",
    "y = df_clean['Churn']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target distribution: {y.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "61ab1e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: 5634 samples (80.0%)\n",
      "Test set: 1409 samples (20.0%)\n",
      "Train target distribution: {'No': 4139, 'Yes': 1495}\n",
      "Test target distribution: {'No': 1035, 'Yes': 374}\n"
     ]
    }
   ],
   "source": [
    "# Perform stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Train target distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Test target distribution: {y_test.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382e112",
   "metadata": {},
   "source": [
    "## Preprocessing Demonstration\n",
    "\n",
    "Let's fit the preprocessor on training data and demonstrate the transformation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fd879c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor fitted successfully\n",
      "Total features after preprocessing: 30 (4 numerical + 26 categorical)\n"
     ]
    }
   ],
   "source": [
    "# Fit the preprocessor on training data\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Get feature names after transformation\n",
    "categorical_feature_names = list(preprocessor.named_transformers_['categorical'].named_steps['encoder'].get_feature_names_out(categorical_cols))\n",
    "numerical_feature_names = numerical_cols\n",
    "all_feature_names = numerical_feature_names + categorical_feature_names\n",
    "\n",
    "print(f\"Preprocessor fitted successfully\")\n",
    "print(f\"Total features after preprocessing: {len(all_feature_names)} ({len(numerical_feature_names)} numerical + {len(categorical_feature_names)} categorical)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e560be",
   "metadata": {},
   "source": [
    "## Save Processed Data\n",
    "\n",
    "Save the processed training and test sets for use in model training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4cc5d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (5634, 19) -> (5634, 30)\n",
      "Test set: (1409, 19) -> (1409, 30)\n",
      "Processed data saved to ../data/processed/\n"
     ]
    }
   ],
   "source": [
    "# Transform both training and test sets\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train.shape} -> {X_train_processed.shape}\")\n",
    "print(f\"Test set: {X_test.shape} -> {X_test_processed.shape}\")\n",
    "\n",
    "# Save processed data to numpy arrays\n",
    "import os\n",
    "processed_dir = \"../data/processed\"\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "np.save(f\"{processed_dir}/X_train_processed.npy\", X_train_processed)\n",
    "np.save(f\"{processed_dir}/X_test_processed.npy\", X_test_processed)\n",
    "np.save(f\"{processed_dir}/y_train.npy\", y_train.values)\n",
    "np.save(f\"{processed_dir}/y_test.npy\", y_test.values)\n",
    "\n",
    "# Save feature names for reference\n",
    "with open(f\"{processed_dir}/feature_names.json\", 'w') as f:\n",
    "    json.dump({\n",
    "        'numerical_features': numerical_cols,\n",
    "        'categorical_features': categorical_cols,\n",
    "        'all_feature_names': all_feature_names\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"Processed data saved to {processed_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b08cb3",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook completed the feature engineering pipeline for the telco churn prediction project:\n",
    "\n",
    "### Key Components:\n",
    "- **Data Cleaning**: Converted TotalCharges to numeric, handled missing values, removed identifier columns\n",
    "- **Feature Preprocessing**: StandardScaler for numerical features, OneHotEncoder for categorical features  \n",
    "- **Pipeline Creation**: Reproducible sklearn pipelines for consistent preprocessing\n",
    "- **Train-Test Split**: Stratified 80/20 split preserving target distribution\n",
    "- **Data Export**: Processed arrays saved to `/data/processed/` for model training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
